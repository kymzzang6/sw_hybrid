{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 import\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 함수 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 방법 1 각 데이터셋을 따로 필터링해서 저장 (파인튜닝용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "\n",
    "# 1. 원하는 클래스 이미지/라벨 데이터 뽑아내는 함수\n",
    "def extract_classes_finetuning(original_path, wanted_class_ids, class_names = ['helmet', 'gloves', 'boots'], filtered_folder=None):\n",
    "    dataset_name = Path(original_path).name\n",
    "    \n",
    "    # filtered_folder가 지정되지 않으면 자동으로 데이터셋별 폴더 생성\n",
    "    if filtered_folder is None:\n",
    "        filtered_folder = f'filtered_{dataset_name}'\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        # 경로 패턴 1: images/train, labels/train\n",
    "        img_dir1 = os.path.join(original_path, 'images', split)\n",
    "        lbl_dir1 = os.path.join(original_path, 'labels', split)\n",
    "        \n",
    "        # 경로 패턴 2: train/images, train/labels\n",
    "        img_dir2 = os.path.join(original_path, split, 'images')\n",
    "        lbl_dir2 = os.path.join(original_path, split, 'labels')\n",
    "        \n",
    "        if os.path.exists(img_dir1) and os.path.exists(lbl_dir1):\n",
    "            img_dir, lbl_dir = img_dir1, lbl_dir1\n",
    "        elif os.path.exists(img_dir2):\n",
    "            img_dir = img_dir2\n",
    "            lbl_dir = lbl_dir2 if os.path.exists(lbl_dir2) else None\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        filtered_img_dir = os.path.join(filtered_folder, split, 'images')\n",
    "        filtered_lbl_dir = os.path.join(filtered_folder, split, 'labels')\n",
    "        os.makedirs(filtered_img_dir, exist_ok=True)\n",
    "        os.makedirs(filtered_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        class_mapping = {old_id: new_id for new_id, old_id in enumerate(wanted_class_ids)}\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        if lbl_dir and os.path.exists(lbl_dir):\n",
    "            for lbl_file in os.listdir(lbl_dir):\n",
    "                if not lbl_file.endswith('.txt'):\n",
    "                    continue\n",
    "                \n",
    "                lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "                \n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                filtered_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if not parts:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id in class_mapping:\n",
    "                        parts[0] = str(class_mapping[class_id])\n",
    "                        filtered_lines.append(' '.join(parts) + '\\n')\n",
    "                \n",
    "                if not filtered_lines:\n",
    "                    continue\n",
    "                \n",
    "                img_name = os.path.splitext(lbl_file)[0]\n",
    "                img_found = False\n",
    "                \n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = os.path.join(img_dir, img_name + ext)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_name = f\"{dataset_name}_{split}_{counter:04d}{ext}\"\n",
    "                        new_lbl_name = f\"{dataset_name}_{split}_{counter:04d}.txt\"\n",
    "                        \n",
    "                        shutil.copy2(img_path, os.path.join(filtered_img_dir, new_img_name))\n",
    "                        \n",
    "                        with open(os.path.join(filtered_lbl_dir, new_lbl_name), 'w') as f:\n",
    "                            f.writelines(filtered_lines)\n",
    "                        \n",
    "                        counter += 1\n",
    "                        img_found = True\n",
    "                        break\n",
    "        \n",
    "        print(f'[{dataset_name}] {split}: {counter-1}개 추출 완료')\n",
    "    \n",
    "    # 데이터 추출 후 자동으로 yaml 파일 생성\n",
    "    create_yaml(filtered_folder, class_names)\n",
    "    \n",
    "    return filtered_folder\n",
    "\n",
    "\n",
    "# 2. data.yaml 파일 생성 함수\n",
    "def create_yaml(filtered_folder, class_names):\n",
    "    \"\"\"\n",
    "    필터링된 데이터셋 폴더에 data.yaml 파일을 생성합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - filtered_folder: 필터링된 데이터셋이 저장된 폴더 경로\n",
    "    - class_names: 클래스 이름 리스트 (예: ['Hardhat', 'Mask'])\n",
    "    \"\"\"\n",
    "    yaml_path = os.path.join(filtered_folder, 'data.yaml')\n",
    "    \n",
    "    # yaml 파일 내용 구성\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(filtered_folder),  # 절대 경로\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(class_names),  # 클래스 개수\n",
    "        'names': class_names  # 클래스 이름 리스트\n",
    "    }\n",
    "    \n",
    "    # yaml 파일 저장\n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "    \n",
    "    print(f'✓ data.yaml 파일 생성 완료: {yaml_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 방법 2 모든 데이터셋 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 원하는 클래스 이미지/라벨 데이터 뽑아내는 함수\n",
    "def extract_classes(original_path, wanted_class_ids, temp_folder='temp_dataset'):\n",
    "    dataset_name = Path(original_path).name\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        # 경로 패턴 1: images/train, labels/train\n",
    "        img_dir1 = os.path.join(original_path, 'images', split)\n",
    "        lbl_dir1 = os.path.join(original_path, 'labels', split)\n",
    "        \n",
    "        # 경로 패턴 2: train/images, train/labels\n",
    "        img_dir2 = os.path.join(original_path, split, 'images')\n",
    "        lbl_dir2 = os.path.join(original_path, split, 'labels')\n",
    "        \n",
    "        if os.path.exists(img_dir1) and os.path.exists(lbl_dir1):\n",
    "            img_dir, lbl_dir = img_dir1, lbl_dir1\n",
    "        elif os.path.exists(img_dir2):\n",
    "            img_dir = img_dir2\n",
    "            lbl_dir = lbl_dir2 if os.path.exists(lbl_dir2) else None\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        temp_img_dir = os.path.join(temp_folder, split, 'images')\n",
    "        temp_lbl_dir = os.path.join(temp_folder, split, 'labels')\n",
    "        os.makedirs(temp_img_dir, exist_ok=True)\n",
    "        os.makedirs(temp_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        class_mapping = {old_id: new_id for new_id, old_id in enumerate(wanted_class_ids)}\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        if lbl_dir and os.path.exists(lbl_dir):\n",
    "            for lbl_file in os.listdir(lbl_dir):\n",
    "                if not lbl_file.endswith('.txt'):\n",
    "                    continue\n",
    "                \n",
    "                lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "                \n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                filtered_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if not parts:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id in class_mapping:\n",
    "                        parts[0] = str(class_mapping[class_id])\n",
    "                        filtered_lines.append(' '.join(parts) + '\\n')\n",
    "                \n",
    "                if not filtered_lines:\n",
    "                    continue\n",
    "                \n",
    "                img_name = os.path.splitext(lbl_file)[0]\n",
    "                img_found = False\n",
    "                \n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = os.path.join(img_dir, img_name + ext)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_name = f\"{dataset_name}_{split}_{counter:04d}{ext}\"\n",
    "                        new_lbl_name = f\"{dataset_name}_{split}_{counter:04d}.txt\"\n",
    "                        \n",
    "                        shutil.copy2(img_path, os.path.join(temp_img_dir, new_img_name))\n",
    "                        \n",
    "                        with open(os.path.join(temp_lbl_dir, new_lbl_name), 'w') as f:\n",
    "                            f.writelines(filtered_lines)\n",
    "                        \n",
    "                        counter += 1\n",
    "                        img_found = True\n",
    "                        break\n",
    "        \n",
    "        print(f'[{dataset_name}] {split}: {counter-1}개 추출 완료')\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# extract_classes('/path/to/dataset1', [0, 1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 폴더 데이터를 최종 폴더로 이동\n",
    "def move_to_final(temp_folder='temp_dataset', final_folder='final_dataset'):\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        temp_img = os.path.join(temp_folder, split, 'images')\n",
    "        temp_lbl = os.path.join(temp_folder, split, 'labels')\n",
    "        \n",
    "        final_img = os.path.join(final_folder, split, 'images')\n",
    "        final_lbl = os.path.join(final_folder, split, 'labels')\n",
    "        \n",
    "        os.makedirs(final_img, exist_ok=True)\n",
    "        os.makedirs(final_lbl, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(temp_img):\n",
    "            for file in os.listdir(temp_img):\n",
    "                shutil.move(os.path.join(temp_img, file), os.path.join(final_img, file))\n",
    "        \n",
    "        if os.path.exists(temp_lbl):\n",
    "            for file in os.listdir(temp_lbl):\n",
    "                shutil.move(os.path.join(temp_lbl, file), os.path.join(final_lbl, file))\n",
    "    \n",
    "    shutil.rmtree(temp_folder)\n",
    "    print(f'최종 폴더 {final_folder}로 이동 완료')\n",
    "\n",
    "# 사용 예시\n",
    "# move_to_final()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 yaml파일 생성\n",
    "def create_final_yaml(final_folder='final_dataset'):\n",
    "    class_names = ['helmet', 'gloves', 'boots']\n",
    "    \n",
    "    yaml_data = {\n",
    "        'path': os.path.abspath(final_folder),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(class_names),\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    yaml_path = os.path.join(final_folder, 'data.yaml')\n",
    "    \n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(yaml_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "    \n",
    "    print(f'YAML 파일 생성 완료: {yaml_path}')\n",
    "    print(f'클래스: {class_names}')\n",
    "\n",
    "# 사용 예시\n",
    "# create_final_yaml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 파인튜닝용 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 경로의 각 폴더 이름이 모두 train, val, test, images, labels로 이루어져야함. 만약 아니라면 폴더명과 data.yaml 변경 필수\n",
    "# 원본 클래스 순서는 무조건 helmet, gloves, boots 순서로 가져오기\n",
    "\n",
    "extract_classes_finetuning(\n",
    "    original_path='./PPE-1',\n",
    "    wanted_class_ids=[0, 3, 2]  # 원본 PPE-1의 데이터셋이 0:헬멧, 3:장갑, 2:신발이라 이렇게 뽑음\n",
    ")\n",
    "\n",
    "extract_classes_finetuning(\n",
    "    original_path='./PPE-2',\n",
    "    wanted_class_ids=[0, 1, 3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 병합된 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 경로 및 클래스 설정 \n",
    "# 원본 경로의 각 폴더 이름이 모두 train, val, test, images, labels로 이루어져야함. 만약 아니라면 폴더명과 data.yaml 변경 필수\n",
    "# 클래스 순서는 무조건 헬멧, 장갑, 안전화 순서로 하기\n",
    "datasets = [\n",
    "    {'path': 'PPE-1', 'classes': [0, 3, 2]},\n",
    "    {'path': 'PPE-2', 'classes': [0, 1, 3]}\n",
    "]\n",
    "\n",
    "# 각 데이터셋에서 원하는 클래스 추출\n",
    "for dataset in datasets:\n",
    "    extract_classes(dataset['path'], dataset['classes'])\n",
    "\n",
    "# 임시 폴더의 모든 데이터를 최종 폴더로 이동\n",
    "move_to_final()\n",
    "\n",
    "# YAML 파일 생성\n",
    "create_final_yaml(final_folder='final_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
