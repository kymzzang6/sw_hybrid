{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 import\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 함수 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 방법 1 각 데이터셋을 따로 필터링해서 저장 (파인튜닝용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "\n",
    "# 1. 원하는 클래스 이미지/라벨 데이터 뽑아내는 함수\n",
    "def extract_classes_finetuning(original_path, wanted_class_ids, class_names = ['helmet', 'gloves', 'boots'], filtered_folder=None):\n",
    "    dataset_name = Path(original_path).name\n",
    "    \n",
    "    # filtered_folder가 지정되지 않으면 자동으로 데이터셋별 폴더 생성\n",
    "    if filtered_folder is None:\n",
    "        filtered_folder = f'dataset/filtered_{dataset_name}'\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        # 경로 패턴 1: images/train, labels/train\n",
    "        img_dir1 = os.path.join(original_path, 'images', split)\n",
    "        lbl_dir1 = os.path.join(original_path, 'labels', split)\n",
    "        \n",
    "        # 경로 패턴 2: train/images, train/labels\n",
    "        img_dir2 = os.path.join(original_path, split, 'images')\n",
    "        lbl_dir2 = os.path.join(original_path, split, 'labels')\n",
    "        \n",
    "        if os.path.exists(img_dir1) and os.path.exists(lbl_dir1):\n",
    "            img_dir, lbl_dir = img_dir1, lbl_dir1\n",
    "        elif os.path.exists(img_dir2):\n",
    "            img_dir = img_dir2\n",
    "            lbl_dir = lbl_dir2 if os.path.exists(lbl_dir2) else None\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        filtered_img_dir = os.path.join(filtered_folder, split, 'images')\n",
    "        filtered_lbl_dir = os.path.join(filtered_folder, split, 'labels')\n",
    "        os.makedirs(filtered_img_dir, exist_ok=True)\n",
    "        os.makedirs(filtered_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        class_mapping = {old_id: new_id for new_id, old_id in enumerate(wanted_class_ids)}\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        if lbl_dir and os.path.exists(lbl_dir):\n",
    "            for lbl_file in os.listdir(lbl_dir):\n",
    "                if not lbl_file.endswith('.txt'):\n",
    "                    continue\n",
    "                \n",
    "                lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "                \n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                filtered_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if not parts:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id in class_mapping:\n",
    "                        parts[0] = str(class_mapping[class_id])\n",
    "                        filtered_lines.append(' '.join(parts) + '\\n')\n",
    "                \n",
    "                if not filtered_lines:\n",
    "                    continue\n",
    "                \n",
    "                img_name = os.path.splitext(lbl_file)[0]\n",
    "                img_found = False\n",
    "                \n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = os.path.join(img_dir, img_name + ext)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_name = f\"{dataset_name}_{split}_{counter:04d}{ext}\"\n",
    "                        new_lbl_name = f\"{dataset_name}_{split}_{counter:04d}.txt\"\n",
    "                        \n",
    "                        shutil.copy2(img_path, os.path.join(filtered_img_dir, new_img_name))\n",
    "                        \n",
    "                        with open(os.path.join(filtered_lbl_dir, new_lbl_name), 'w') as f:\n",
    "                            f.writelines(filtered_lines)\n",
    "                        \n",
    "                        counter += 1\n",
    "                        img_found = True\n",
    "                        break\n",
    "        \n",
    "        print(f'[{dataset_name}] {split}: {counter-1}개 추출 완료')\n",
    "    \n",
    "    # 데이터 추출 후 자동으로 yaml 파일 생성\n",
    "    create_yaml(filtered_folder, class_names)\n",
    "    \n",
    "    return filtered_folder\n",
    "\n",
    "\n",
    "# 2. data.yaml 파일 생성 함수\n",
    "def create_yaml(filtered_folder, class_names):\n",
    "    \"\"\"\n",
    "    필터링된 데이터셋 폴더에 data.yaml 파일을 생성합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - filtered_folder: 필터링된 데이터셋이 저장된 폴더 경로\n",
    "    - class_names: 클래스 이름 리스트 (예: ['Hardhat', 'Mask'])\n",
    "    \"\"\"\n",
    "    yaml_path = os.path.join(filtered_folder, 'data.yaml')\n",
    "    \n",
    "    # yaml 파일 내용 구성\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(filtered_folder),  # 절대 경로\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(class_names),  # 클래스 개수\n",
    "        'names': class_names  # 클래스 이름 리스트\n",
    "    }\n",
    "    \n",
    "    # yaml 파일 저장\n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "    \n",
    "    print(f'✓ data.yaml 파일 생성 완료: {yaml_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets_yaml(yaml_paths, output_folder, combined_name='combined_dataset'):\n",
    "    \"\"\"\n",
    "    여러 데이터셋의 yaml 파일을 읽어서 하나의 combined yaml 파일을 생성합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    - yaml_paths: 결합할 데이터셋들의 yaml 파일 경로 리스트 \n",
    "                  (예: ['dataset1/data.yaml', 'dataset2/data.yaml'])\n",
    "    - output_folder: 결합된 yaml 파일을 저장할 폴더 경로\n",
    "    - combined_name: 결합된 yaml 파일 이름 (기본값: 'combined_dataset')\n",
    "    \n",
    "    Returns:\n",
    "    - combined_yaml_path: 생성된 combined yaml 파일의 경로\n",
    "    \"\"\"\n",
    "    \n",
    "    # 출력 폴더 생성\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # 각 데이터셋의 경로 정보를 저장할 리스트\n",
    "    train_paths = []\n",
    "    val_paths = []\n",
    "    test_paths = []\n",
    "    all_class_names = []\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('데이터셋 결합 시작...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # 각 yaml 파일 읽기\n",
    "    for i, yaml_path in enumerate(yaml_paths, 1):\n",
    "        print(f'\\n[{i}/{len(yaml_paths)}] 읽는 중: {yaml_path}')\n",
    "        \n",
    "        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        # 데이터셋의 절대 경로\n",
    "        dataset_path = data['path']\n",
    "        \n",
    "        # train/val/test 경로 추가 (절대 경로로 변환)\n",
    "        train_paths.append(os.path.join(dataset_path, data['train']))\n",
    "        val_paths.append(os.path.join(dataset_path, data['val']))\n",
    "        \n",
    "        if 'test' in data and data['test']:\n",
    "            test_paths.append(os.path.join(dataset_path, data['test']))\n",
    "        \n",
    "        # 클래스 이름 수집 (중복 제거)\n",
    "        for class_name in data['names']:\n",
    "            if class_name not in all_class_names:\n",
    "                all_class_names.append(class_name)\n",
    "        \n",
    "        print(f'  ✓ 클래스 수: {data[\"nc\"]}개')\n",
    "        print(f'  ✓ 클래스 이름: {data[\"names\"]}')\n",
    "    \n",
    "    # Combined yaml 내용 구성\n",
    "    combined_yaml_content = {\n",
    "        'path': '',  # 여러 경로를 사용하므로 비워둠\n",
    "        'train': train_paths,  # 리스트 형태로 여러 경로 지정\n",
    "        'val': val_paths,      # 리스트 형태로 여러 경로 지정\n",
    "        'nc': len(all_class_names),  # 통합된 클래스 개수\n",
    "        'names': all_class_names      # 통합된 클래스 이름 리스트\n",
    "    }\n",
    "    \n",
    "    # test 경로가 있으면 추가\n",
    "    if test_paths:\n",
    "        combined_yaml_content['test'] = test_paths\n",
    "    \n",
    "    # Combined yaml 파일 저장\n",
    "    combined_yaml_path = os.path.join(output_folder, f'{combined_name}.yaml')\n",
    "    with open(combined_yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(combined_yaml_content, f, default_flow_style=False, \n",
    "                  allow_unicode=True, sort_keys=False)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('✓ 데이터셋 결합 완료!')\n",
    "    print('=' * 60)\n",
    "    print(f'생성된 파일: {combined_yaml_path}')\n",
    "    print(f'\\n[결합된 데이터셋 정보]')\n",
    "    print(f'- 총 데이터셋 수: {len(yaml_paths)}개')\n",
    "    print(f'- Train 경로 수: {len(train_paths)}개')\n",
    "    print(f'- Val 경로 수: {len(val_paths)}개')\n",
    "    if test_paths:\n",
    "        print(f'- Test 경로 수: {len(test_paths)}개')\n",
    "    print(f'- 총 클래스 수: {len(all_class_names)}개')\n",
    "    print(f'- 클래스 이름: {all_class_names}')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    return combined_yaml_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 방법 2 모든 데이터셋 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 원하는 클래스 이미지/라벨 데이터 뽑아내는 함수\n",
    "def extract_classes(original_path, wanted_class_ids, temp_folder='temp_dataset'):\n",
    "    dataset_name = Path(original_path).name\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        # 경로 패턴 1: images/train, labels/train\n",
    "        img_dir1 = os.path.join(original_path, 'images', split)\n",
    "        lbl_dir1 = os.path.join(original_path, 'labels', split)\n",
    "        \n",
    "        # 경로 패턴 2: train/images, train/labels\n",
    "        img_dir2 = os.path.join(original_path, split, 'images')\n",
    "        lbl_dir2 = os.path.join(original_path, split, 'labels')\n",
    "        \n",
    "        if os.path.exists(img_dir1) and os.path.exists(lbl_dir1):\n",
    "            img_dir, lbl_dir = img_dir1, lbl_dir1\n",
    "        elif os.path.exists(img_dir2):\n",
    "            img_dir = img_dir2\n",
    "            lbl_dir = lbl_dir2 if os.path.exists(lbl_dir2) else None\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        temp_img_dir = os.path.join(temp_folder, split, 'images')\n",
    "        temp_lbl_dir = os.path.join(temp_folder, split, 'labels')\n",
    "        os.makedirs(temp_img_dir, exist_ok=True)\n",
    "        os.makedirs(temp_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        class_mapping = {old_id: new_id for new_id, old_id in enumerate(wanted_class_ids)}\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        if lbl_dir and os.path.exists(lbl_dir):\n",
    "            for lbl_file in os.listdir(lbl_dir):\n",
    "                if not lbl_file.endswith('.txt'):\n",
    "                    continue\n",
    "                \n",
    "                lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "                \n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                filtered_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if not parts:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id in class_mapping:\n",
    "                        parts[0] = str(class_mapping[class_id])\n",
    "                        filtered_lines.append(' '.join(parts) + '\\n')\n",
    "                \n",
    "                if not filtered_lines:\n",
    "                    continue\n",
    "                \n",
    "                img_name = os.path.splitext(lbl_file)[0]\n",
    "                img_found = False\n",
    "                \n",
    "                for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
    "                    img_path = os.path.join(img_dir, img_name + ext)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_name = f\"{dataset_name}_{split}_{counter:04d}{ext}\"\n",
    "                        new_lbl_name = f\"{dataset_name}_{split}_{counter:04d}.txt\"\n",
    "                        \n",
    "                        shutil.copy2(img_path, os.path.join(temp_img_dir, new_img_name))\n",
    "                        \n",
    "                        with open(os.path.join(temp_lbl_dir, new_lbl_name), 'w') as f:\n",
    "                            f.writelines(filtered_lines)\n",
    "                        \n",
    "                        counter += 1\n",
    "                        img_found = True\n",
    "                        break\n",
    "        \n",
    "        print(f'[{dataset_name}] {split}: {counter-1}개 추출 완료')\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# extract_classes('/path/to/dataset1', [0, 1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 폴더 데이터를 최종 폴더로 이동\n",
    "def move_to_final(temp_folder='temp_dataset', final_folder='final_dataset'):\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        temp_img = os.path.join(temp_folder, split, 'images')\n",
    "        temp_lbl = os.path.join(temp_folder, split, 'labels')\n",
    "        \n",
    "        final_img = os.path.join(final_folder, split, 'images')\n",
    "        final_lbl = os.path.join(final_folder, split, 'labels')\n",
    "        \n",
    "        os.makedirs(final_img, exist_ok=True)\n",
    "        os.makedirs(final_lbl, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(temp_img):\n",
    "            for file in os.listdir(temp_img):\n",
    "                shutil.move(os.path.join(temp_img, file), os.path.join(final_img, file))\n",
    "        \n",
    "        if os.path.exists(temp_lbl):\n",
    "            for file in os.listdir(temp_lbl):\n",
    "                shutil.move(os.path.join(temp_lbl, file), os.path.join(final_lbl, file))\n",
    "    \n",
    "    shutil.rmtree(temp_folder)\n",
    "    print(f'최종 폴더 {final_folder}로 이동 완료')\n",
    "\n",
    "# 사용 예시\n",
    "# move_to_final()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 yaml파일 생성\n",
    "def create_final_yaml(final_folder='final_dataset'):\n",
    "    class_names = ['helmet', 'gloves', 'boots']\n",
    "    \n",
    "    yaml_data = {\n",
    "        'path': os.path.abspath(final_folder),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(class_names),\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    yaml_path = os.path.join(final_folder, 'data.yaml')\n",
    "    \n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(yaml_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "    \n",
    "    print(f'YAML 파일 생성 완료: {yaml_path}')\n",
    "    print(f'클래스: {class_names}')\n",
    "\n",
    "# 사용 예시\n",
    "# create_final_yaml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 파인튜닝용 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 경로의 각 폴더 이름이 모두 train, val, test, images, labels로 이루어져야함. 만약 아니라면 폴더명과 data.yaml 변경 필수\n",
    "# 원본 클래스 순서는 무조건 helmet, gloves, boots 순서로 가져오기\n",
    "\n",
    "extract_classes_finetuning(\n",
    "    original_path='./dataset/PPE-1',\n",
    "    class_names = ['helmet', 'vest', 'gloves', 'boots'],\n",
    "    wanted_class_ids=[0, 5, 3, 2],  # 원본 PPE-1의 데이터셋이 0:헬멧, 3:장갑, 2:신발, 5:vest라 이렇게 뽑음\n",
    ")\n",
    "\n",
    "extract_classes_finetuning(\n",
    "    original_path='./dataset/PPE-2',\n",
    "    class_names = ['helmet', 'vest', 'gloves', 'boots'],\n",
    "    wanted_class_ids=[0, 2, 1, 3],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 병합된 데이터셋 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "yaml파일 병합으로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋1과 데이터셋2의 yaml 파일 경로\n",
    "yaml_files = [\n",
    "    './dataset/filtered_PPE-1/data.yaml',\n",
    "    './dataset/filtered_PPE-2/data.yaml'\n",
    "]\n",
    "\n",
    "# Combined yaml 파일 생성\n",
    "combined_yaml = combine_datasets_yaml(\n",
    "    yaml_paths=yaml_files,\n",
    "    output_folder='./dataset/',\n",
    "    combined_name='ppe_combined'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "데이터 자체 병합으로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 경로 및 클래스 설정 \n",
    "# 원본 경로의 각 폴더 이름이 모두 train, val, test, images, labels로 이루어져야함. 만약 아니라면 폴더명과 data.yaml 변경 필수\n",
    "# 클래스 순서는 무조건 헬멧, 장갑, 안전화 순서로 하기\n",
    "datasets = [\n",
    "    {'path': 'PPE-1', 'classes': [0, 3, 2]},\n",
    "    {'path': 'PPE-2', 'classes': [0, 1, 3]}\n",
    "]\n",
    "\n",
    "# 각 데이터셋에서 원하는 클래스 추출\n",
    "for dataset in datasets:\n",
    "    extract_classes(dataset['path'], dataset['classes'])\n",
    "\n",
    "# 임시 폴더의 모든 데이터를 최종 폴더로 이동\n",
    "move_to_final()\n",
    "\n",
    "# YAML 파일 생성\n",
    "create_final_yaml(final_folder='dataset/final_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
